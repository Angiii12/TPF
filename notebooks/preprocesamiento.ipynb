{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b33cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, RobustScaler, FunctionTransformer\n",
    "from sklearn.linear_model import LassoCV, Ridge, Lasso, ElasticNetCV\n",
    "from sklearn.feature_selection import f_regression, RFE, SelectKBest, SequentialFeatureSelector, RFECV, VarianceThreshold, mutual_info_regression, SelectFromModel\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a925fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(\"..\", \"data\", \"dataset_preprocesamiento_train.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"..\", \"data\", \"dataset_preprocesamiento_test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8b869",
   "metadata": {},
   "source": [
    "## Fase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4152d8",
   "metadata": {},
   "source": [
    "### Creación de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6288c036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inicial: (913, 118) | Test inicial: (297, 118)\n",
      "Top 5 features aprendidas: ['Sala Maq (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EE', 'KW Gral Planta__Consolidado EE', 'Planta (Kw)__Consolidado EE', 'Agua Planta (Hl)__Consolidado Agua']\n",
      "DataFrame unificado para procesar: (1210, 118)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train inicial: {df_train.shape} | Test inicial: {df_test.shape}\")\n",
    "\n",
    "ycol = 'Frio'\n",
    "\n",
    "num_train = df_train.select_dtypes(include=[np.number]).copy()\n",
    "candidatas_train = [c for c in num_train.columns if c != ycol and c != 'y' and num_train[c].notna().any()]\n",
    "\n",
    "top5_features_aprendidas = (\n",
    "    num_train[candidatas_train].corrwith(num_train[ycol])\n",
    "    .abs()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(5)\n",
    "    .index.tolist())\n",
    "\n",
    "print(f\"Top 5 features aprendidas: {top5_features_aprendidas}\")\n",
    "\n",
    "\n",
    "split_date = df_test['dia'].min() # Guardamos la fecha de corte\n",
    "df_full = pd.concat([df_train, df_test]).sort_values(by='dia').reset_index(drop=True)\n",
    "\n",
    "print(f\"DataFrame unificado para procesar: {df_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306357e7",
   "metadata": {},
   "source": [
    "#### Lags y Rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a013fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lags y Rolling (Tus cambios) ---\n",
    "ycol = 'Frio'\n",
    "df_full[f'{ycol}_lag7'] = df_full[ycol].shift(7)\n",
    "\n",
    "y_obs = df_full[ycol].shift(1) # Frio de ayer\n",
    "df_full[f'{ycol}_ma3'] = y_obs.rolling(window=3, min_periods=1).mean()\n",
    "df_full[f'{ycol}_ma7'] = y_obs.rolling(window=7, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bbcc0",
   "metadata": {},
   "source": [
    "#### Fechas y findes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9336bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fechas Cíclicas y Finde ---\n",
    "df_full['fecha'] = pd.to_datetime(df_full['dia'], format='%Y-%m-%d', errors='coerce')\n",
    "dow = df_full['fecha'].dt.dayofweek\n",
    "df_full['dow_sin'] = np.sin(2*np.pi * dow / 7)\n",
    "df_full['dow_cos'] = np.cos(2*np.pi * dow / 7)\n",
    "m  = df_full['fecha'].dt.month\n",
    "m0 = (m - 1)\n",
    "df_full['mes_sin'] = np.sin(2*np.pi * m0 / 12)\n",
    "df_full['mes_cos'] = np.cos(2*np.pi * m0 / 12)\n",
    "df_full['fin_de_semana'] = df_full['fecha'].dt.dayofweek.isin([5, 6]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9005a71",
   "metadata": {},
   "source": [
    "#### Ratios e interacciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcca97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ratios e Interacciones (usando la 'top5_features_aprendidas') ---\n",
    "top5 = top5_features_aprendidas # Usamos la variable segura\n",
    "for a, b in combinations(top5, 2):\n",
    "    df_full[f'{a}x{b}'] = df_full[a] * df_full[b]\n",
    "for cyc in ['dow_sin', 'dow_cos', 'mes_sin', 'mes_cos']:\n",
    "    if cyc in df_full.columns:\n",
    "        for c in top5:\n",
    "            df_full[f'{c}_x_{cyc}'] = df_full[c] * df_full[cyc]\n",
    "if 'fin_de_semana' in df_full.columns:\n",
    "    for c in top5:\n",
    "        df_full[f'{c}_x_finde'] = df_full[c] * df_full['fin_de_semana']\n",
    "\n",
    "# --- Ratios de Áreas ---\n",
    "AREAS = {\n",
    "    \"Elaboración\": r\"Elab|Elabor|Coci|Cocina|Mosto|Lauter|Macer|Paste\",\n",
    "    \"Envasado\":    r\"Envas|Llen|Linea|L[2345]\\b\",\n",
    "    \"Bodega\":      r\"Bodega|Bodeg\",\n",
    "    \"Servicios\":   r\"Servicios|Vapor|Gas|Agua|Aire|Caldera|Compres|Chiller|Sala\",\n",
    "    \"Sala_Maq\":    r\"Sala.*Maq\",\n",
    "}\n",
    "def safe_div(a, b):\n",
    "    # Usamos np.where para evitar dividir por cero\n",
    "    return np.where(b != 0, a / b, 0)\n",
    "\n",
    "num_cols = df_full.select_dtypes(include=[np.number]).columns\n",
    "area_cols = {}\n",
    "for area, pat in AREAS.items():\n",
    "    regex = re.compile(pat, flags=re.IGNORECASE)\n",
    "    cols = [c for c in num_cols if regex.search(c)]\n",
    "    area_cols[area] = cols\n",
    "\n",
    "area_sum = {}\n",
    "for area, cols in area_cols.items():\n",
    "    if cols:\n",
    "        df_full[f'{area}_sum'] = df_full[cols].sum(axis=1, skipna=True)\n",
    "        area_sum[area] = df_full[f'{area}_sum']\n",
    "    else:\n",
    "        df_full[f'{area}_sum'] = 0.0\n",
    "        area_sum[area] = df_full[f'{area}_sum']\n",
    "\n",
    "areas_presentes = list(area_sum.keys())\n",
    "total_sel = sum(area_sum[a] for a in areas_presentes)\n",
    "df_full['Consumo_Total_Areas'] = total_sel\n",
    "\n",
    "for a in areas_presentes:\n",
    "    df_full[f'{a}_share'] = safe_div(df_full[f'{a}_sum'], df_full['Consumo_Total_Areas'])\n",
    "for a, b in combinations(areas_presentes, 2):\n",
    "    df_full[f'ratio_{a}_sobre_{b}'] = safe_div(df_full[f'{a}_sum'], df_full[f'{b}_sum'])\n",
    "    df_full[f'ratio_{b}_sobre_{a}'] = safe_div(df_full[f'{b}_sum'], df_full[f'{a}_sum'])\n",
    "\n",
    "df_full = df_full.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297157d",
   "metadata": {},
   "source": [
    "#### Estacionalidad y clima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02300f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando clima de 2020-07-01 a 2023-10-25...\n"
     ]
    }
   ],
   "source": [
    "# --- Estacionalidad y Clima ---\n",
    "m = df_full['fecha'].dt.month\n",
    "est_idx = np.select(\n",
    "    [m.isin([12, 1, 2]), m.isin([3, 4, 5]), m.isin([6, 7, 8]), m.isin([9, 10, 11])],\n",
    "    [0, 1, 2, 3], default=np.nan\n",
    ")\n",
    "df_full['estacion_sin'] = np.sin(2*np.pi * est_idx / 4)\n",
    "df_full['estacion_cos'] = np.cos(2*np.pi * est_idx / 4)\n",
    "\n",
    "start_date = df_full[\"fecha\"].min().date().isoformat()\n",
    "end_date   = df_full[\"fecha\"].max().date().isoformat()\n",
    "print(f\"Buscando clima de {start_date} a {end_date}...\")\n",
    "lat, lon = 32.56717, -116.62509\n",
    "url = (\n",
    "    \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    f\"?latitude={lat}&longitude={lon}\"\n",
    "    f\"&start_date={start_date}&end_date={end_date}\"\n",
    "    \"&daily=temperature_2m_mean\"\n",
    "    \"&timezone=auto\"\n",
    ")\n",
    "data = requests.get(url, timeout=60).json()\n",
    "wx_d = pd.DataFrame({\n",
    "    \"fecha\": pd.to_datetime(data[\"daily\"][\"time\"]),\n",
    "    \"t2m_mean_C\": data[\"daily\"][\"temperature_2m_mean\"],\n",
    "})\n",
    "df_full = df_full.merge(wx_d, on=\"fecha\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef995d26",
   "metadata": {},
   "source": [
    "#### Limpiar y separar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca6fcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas eliminadas por NaNs de lags: 7. Quedan 1203 filas.\n"
     ]
    }
   ],
   "source": [
    "df_processed = df_full.iloc[7:].copy()\n",
    "print(f\"Filas eliminadas por NaNs de lags: 7. Quedan {df_processed.shape[0]} filas.\")\n",
    "\n",
    "# 3b. Rellenar cualquier otro NaN que quede (del merge de clima, ratios, etc.)\n",
    "df_processed = df_processed.fillna(0) # O puedes usar un imputador aquí si prefieres\n",
    "\n",
    "# 3c. Volver a separar\n",
    "df_train_final = df_processed[df_processed['dia'] < split_date].copy()\n",
    "df_test_final = df_processed[df_processed['dia'] >= split_date].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7857db",
   "metadata": {},
   "source": [
    "### Selección de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb317c",
   "metadata": {},
   "source": [
    "#### Separamos en x, y train y x, y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7d256ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listos para seleccionar. 192 features numéricas iniciales.\n"
     ]
    }
   ],
   "source": [
    "ycol = 'y'\n",
    "\n",
    "# --- Creamos X_train, y_train, X_test, y_test ---\n",
    "# (Esta vez, X_train y X_test se quedan con las columnas de texto por ahora)\n",
    "X_train_full = df_train_final.drop(columns=[ycol], errors='ignore')\n",
    "y_train = df_train_final[ycol]\n",
    "\n",
    "X_test_full = df_test_final.drop(columns=[ycol], errors='ignore')\n",
    "y_test = df_test_final[ycol]\n",
    "\n",
    "# 1. Creamos una lista de features que SÍ son numéricas\n",
    "numeric_features = X_train_full.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# 2. Creamos los DataFrames X_train y X_test solo con esas columnas\n",
    "X_train = X_train_full[numeric_features].copy()\n",
    "X_test = X_test_full[numeric_features].copy()\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "print(f\"Listos para seleccionar. {len(numeric_features)} features numéricas iniciales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389cce2c",
   "metadata": {},
   "source": [
    "#### Métodos de varianza, F-Score y Mutual Info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "319eaae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Método Varianza (k=30) --- \n",
      "['Unnamed: 0', 'Linea 3 (Kw)__Consolidado EE', 'Linea 2 (Kw)__Consolidado EE', 'Vapor L3__Consolidado GasVapor', 'Totalizador_Aire_L3__Consolidado Aire', 'Totalizador_Aire_L5__Consolidado Aire', 'dow_sin', 'dow_cos', 'mes_sin', 'mes_cos', 'fin_de_semana', 'KW Gral Planta__Consolidado EE_x_dow_sin', 'Planta (Kw)__Consolidado EE_x_dow_sin', 'Servicios (Kw)__Consolidado EE_x_dow_cos', 'KW Gral Planta__Consolidado EE_x_dow_cos', 'Planta (Kw)__Consolidado EE_x_dow_cos', 'Agua Planta (Hl)__Consolidado Agua_x_dow_cos', 'Servicios (Kw)__Consolidado EE_x_mes_sin', 'KW Gral Planta__Consolidado EE_x_mes_sin', 'Planta (Kw)__Consolidado EE_x_mes_sin', 'Agua Planta (Hl)__Consolidado Agua_x_mes_sin', 'KW Gral Planta__Consolidado EE_x_mes_cos', 'Planta (Kw)__Consolidado EE_x_mes_cos', 'Sala Maq (Kw)__Consolidado EE_x_finde', 'Servicios (Kw)__Consolidado EE_x_finde', 'KW Gral Planta__Consolidado EE_x_finde', 'Planta (Kw)__Consolidado EE_x_finde', 'Agua Planta (Hl)__Consolidado Agua_x_finde', 'estacion_sin', 'estacion_cos']\n",
      "\n",
      "--- 2. Método F-Score (k=30) --- \n",
      "['Sala Maq (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EE', 'KW Gral Planta__Consolidado EE', 'Planta (Kw)__Consolidado EE', 'Agua Planta (Hl)__Consolidado Agua', 'Frio', 'Frio_lag7', 'Frio_ma3', 'Frio_ma7', 'mes_cos', 'Sala Maq (Kw)__Consolidado EExServicios (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Servicios (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'Servicios (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'KW Gral Planta__Consolidado EExPlanta (Kw)__Consolidado EE', 'KW Gral Planta__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Planta (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Servicios (Kw)__Consolidado EE_x_mes_cos', 'KW Gral Planta__Consolidado EE_x_mes_cos', 'Agua Planta (Hl)__Consolidado Agua_x_mes_cos', 'Servicios_sum', 'Sala_Maq_sum', 'Consumo_Total_Areas', 'Bodega_share', 'ratio_Bodega_sobre_Servicios', 'ratio_Servicios_sobre_Bodega', 'ratio_Sala_Maq_sobre_Bodega']\n",
      "\n",
      "--- 3. Método Mutual Info (k=30) --- \n",
      "['Unnamed: 0', 'Sala Maq (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EE', 'KW Gral Planta__Consolidado EE', 'Planta (Kw)__Consolidado EE', 'Frio', 'Frio_ma3', 'Frio_ma7', 'Sala Maq (Kw)__Consolidado EExServicios (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Servicios (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'Servicios (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'KW Gral Planta__Consolidado EExPlanta (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EE_x_dow_cos', 'Servicios (Kw)__Consolidado EE_x_dow_cos', 'Sala Maq (Kw)__Consolidado EE_x_mes_cos', 'Servicios (Kw)__Consolidado EE_x_mes_cos', 'KW Gral Planta__Consolidado EE_x_mes_cos', 'Planta (Kw)__Consolidado EE_x_mes_cos', 'Servicios_sum', 'Sala_Maq_sum', 'Consumo_Total_Areas', 'Bodega_share', 'ratio_Bodega_sobre_Servicios', 'ratio_Servicios_sobre_Bodega', 'ratio_Bodega_sobre_Sala_Maq', 'ratio_Sala_Maq_sobre_Bodega']\n"
     ]
    }
   ],
   "source": [
    "# --- A. Filtro por Varianza (VarianceThreshold) ---\n",
    "scaler_var = MinMaxScaler()\n",
    "scaler_var.fit(X_train) \n",
    "\n",
    "X_train_scaled_var = pd.DataFrame(scaler_var.transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled_var = pd.DataFrame(scaler_var.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "var_selector = SelectKBest(score_func=lambda X, y: X.var(axis=0), k=30)\n",
    "var_selector.fit(X_train_scaled_var, y_train) # 'y_train' no se usa, pero la API lo pide\n",
    "\n",
    "features_variance = X_train.columns[var_selector.get_support()]\n",
    "print(f\"\\n--- 1. Método Varianza (k=30) --- \\n{features_variance.tolist()}\")\n",
    "\n",
    "\n",
    "# --- B. Filtro por F-Score (f_regression) ---\n",
    "X_train_scaled_var = X_train_scaled_var.fillna(0)\n",
    "X_test_scaled_var = X_test_scaled_var.fillna(0)\n",
    "\n",
    "f_selector = SelectKBest(score_func=f_regression, k=30)\n",
    "f_selector.fit(X_train_scaled_var, y_train)\n",
    "\n",
    "features_fscore = X_train.columns[f_selector.get_support()]\n",
    "print(f\"\\n--- 2. Método F-Score (k=30) --- \\n{features_fscore.tolist()}\")\n",
    "\n",
    "\n",
    "# --- C. Filtro por Información Mutua (mutual_info_regression) ---\n",
    "mi_selector = SelectKBest(score_func=mutual_info_regression, k=30)\n",
    "mi_selector.fit(X_train_scaled_var, y_train)\n",
    "\n",
    "features_mi = X_train.columns[mi_selector.get_support()]\n",
    "print(f\"\\n--- 3. Método Mutual Info (k=30) --- \\n{features_mi.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a12060",
   "metadata": {},
   "source": [
    "#### Método de Lasso y ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b83624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.957e+06, tolerance: 4.830e+06\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.414e+06, tolerance: 4.830e+06\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.720e+06, tolerance: 4.830e+06\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.107e+06, tolerance: 5.007e+06\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Método Lasso (top 30) --- \n",
      "['Frio', 'Frio_ma7', 'mes_cos', 'ratio_Bodega_sobre_Sala_Maq', 'ratio_Servicios_sobre_Bodega', 'mes_sin', 'estacion_cos', 'Red Paste L4__Consolidado Agua', 'ET Servicios / Hl__Consolidado KPI', 'Sala Maq (Kw)__Consolidado EE_x_dow_cos', 'Aire Envasado (M3)__Consolidado Aire', 'Agua Bodega / Hl__Consolidado KPI', 'KW Gral Planta__Consolidado EE', 'Aire Bodega / Hl__Consolidado KPI', 'Sala Maq (Kw)__Consolidado EE', 'Frio_lag7', 'Servicios (Kw)__Consolidado EE', 'Agua Planta (Hl)__Consolidado Agua_x_dow_sin', 'Planta (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Servicios (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'KW Gral Planta__Consolidado EE_x_dow_cos', 'KW Gral Planta__Consolidado EExPlanta (Kw)__Consolidado EE', 'KW Gral Planta__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Sala Maq (Kw)__Consolidado EE_x_dow_sin', 'Servicios (Kw)__Consolidado EE_x_dow_cos', 'Planta (Kw)__Consolidado EE_x_dow_sin', 'Servicios (Kw)__Consolidado EE_x_dow_sin', 'KW Gral Planta__Consolidado EE_x_dow_sin', 'Sala Maq (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua']\n",
      "\n",
      "--- 5. Método ElasticNet (top 30) --- \n",
      "['Frio_ma7', 'Frio_ma3', 'mes_cos', 'estacion_cos', 'Frio_lag7', 'Frio', 'Servicios (Kw)__Consolidado EE_x_mes_cos', 'KW Gral Planta__Consolidado EE_x_mes_cos', 'Agua Planta (Hl)__Consolidado Agua_x_mes_cos', 'Planta (Kw)__Consolidado EE_x_mes_cos', 'Sala Maq (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EE_x_mes_cos', 'ratio_Sala_Maq_sobre_Bodega', 'Sala Maq (Kw)__Consolidado EExServicios (Kw)__Consolidado EE', 'ratio_Servicios_sobre_Bodega', 'Sala Maq (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'Sala_Maq_sum', 'Sala Maq (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EE', 'ratio_Sala_Maq_sobre_Servicios', 'Consumo_Total_Areas', 'Sala_Maq_share', 'Servicios_share', 'Bodega_share', 'Servicios (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'ratio_Bodega_sobre_Sala_Maq', 'Sala Maq (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Servicios_sum', 'Servicios (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 't2m_mean_C']\n"
     ]
    }
   ],
   "source": [
    "# --- D. Método Embebido (LassoCV) ---\n",
    "scaler_reg = StandardScaler()\n",
    "\n",
    "scaler_reg.fit(X_train)\n",
    "\n",
    "X_train_scaled_reg = pd.DataFrame(scaler_reg.transform(X_train.fillna(0)), columns=X_train.columns)\n",
    "X_test_scaled_reg = pd.DataFrame(scaler_reg.transform(X_test.fillna(0)), columns=X_test.columns)\n",
    "\n",
    "lasso = LassoCV(cv=5, random_state=42).fit(X_train_scaled_reg, y_train)\n",
    "\n",
    "coef_lasso = pd.Series(lasso.coef_, index=X_train.columns)\n",
    "features_lasso = coef_lasso.abs().sort_values(ascending=False).head(30).index.tolist()\n",
    "print(f\"\\n--- 4. Método Lasso (top 30) --- \\n{features_lasso}\")\n",
    "\n",
    "# --- E. Método Embebido (ElasticNetCV) ---\n",
    "elastic = ElasticNetCV(cv=5, random_state=42).fit(X_train_scaled_reg, y_train)\n",
    "\n",
    "coef_elastic = pd.Series(elastic.coef_, index=X_train.columns)\n",
    "features_elastic = coef_elastic.abs().sort_values(ascending=False).head(30).index.tolist()\n",
    "print(f\"\\n--- 5. Método ElasticNet (top 30) --- \\n{features_elastic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7fc649",
   "metadata": {},
   "source": [
    "#### SequencialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d5c6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- F. Método Wrapper (SequentialFeatureSelector) ---\n",
    "# # (Usamos los datos escalados con StandardScaler, que ya están limpios de NaNs)\n",
    "# print(\"\\n--- 6. Método Wrapper (SFS) ---\")\n",
    "# print(\"Iniciando SFS... (Esto puede tardar varios minutos)...\")\n",
    "\n",
    "# # 1. Definir el modelo que usará SFS para evaluar\n",
    "# # (Tu código usaba GradientBoosting, es una buena elección)\n",
    "# modelo_sfs = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# # 2. APRENDER (fit) el selector SFS SOLO en X_train\n",
    "# sfs = SequentialFeatureSelector(modelo_sfs, \n",
    "#                               n_features_to_select=30, # ¡Aquí pones 30!\n",
    "#                               direction='forward', # 'forward' es más rápido que 'backward'\n",
    "#                               cv=3, # 3 splits es más rápido que 5\n",
    "#                               n_jobs=-1) # Usar todos los cores\n",
    "\n",
    "# sfs.fit(X_train_scaled_reg, y_train)\n",
    "\n",
    "# # 3. OBTENER las 30 mejores features\n",
    "# features_sfs = X_train.columns[sfs.get_support()]\n",
    "# print(f\"SFS completado.\\nFeatures seleccionadas: \\n{features_sfs.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3d2d5",
   "metadata": {},
   "source": [
    "--- 6. Método Wrapper (SFS) ---\n",
    "Features seleccionadas por SFS: \n",
    "\n",
    "\n",
    "['Agua Elab / Hl__Consolidado KPI', 'Agua Planta de Agua/Hl__Consolidado KPI', 'Produccion Agua / Hl__Consolidado KPI', 'EE Envasado / Hl__Consolidado KPI', 'Agua Bodega / Hl__Consolidado KPI', 'EE Elaboracion / Hl__Consolidado KPI', 'EE Linea 4 / Hl__Consolidado KPI', 'Aire Elaboracion / Hl__Consolidado KPI', 'KW Enfluentes Hidr__Totalizadores Energia', 'KW Obrador Contratistas__Totalizadores Energia', 'KW Mycom 3__Totalizadores Energia', 'KW Mycom 7__Totalizadores Energia', 'FC Lavadora L3__Consolidado Agua', 'Produccion (Hl)__Consolidado Agua', 'Frio', 'Frio_lag7', 'Frio_ma7', 'dow_sin', 'dow_cos', 'mes_sin', 'fin_de_semana', 'Servicios (Kw)__Consolidado EE_x_dow_sin', 'KW Gral Planta__Consolidado EE_x_dow_sin', 'KW Gral Planta__Totalizadores Energia_x_dow_sin', 'Planta (Kw)__Consolidado EE_x_dow_sin', 'KW Gral Planta__Consolidado EE_x_dow_cos', 'KW Gral Planta__Totalizadores Energia_x_dow_cos', 'Servicios (Kw)__Consolidado EE_x_finde', 'Bodega_sum', 'estacion_sin']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8d70e",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62caa247",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "perm_train = permutation_importance(\n",
    "    rf, X_train, y_train,\n",
    "    n_repeats=5, random_state=42, n_jobs=-1,\n",
    "    scoring=\"neg_mean_absolute_error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "866c1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_imp_train = pd.Series(perm_train.importances_mean, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "features_rf = perm_imp_train.head(30).index.tolist()\n",
    "\n",
    "X_train_sel = X_train[features_rf].copy()\n",
    "X_test_sel  = X_test[features_rf].copy()\n",
    "\n",
    "rf_sel = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "rf_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "y_pred = rf_sel.predict(X_test_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2842d9c",
   "metadata": {},
   "source": [
    "#### Comparación de métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f32a6893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Varianza (30 features) <<<\n",
      "  R2 (en Test):   0.1733\n",
      "  RMSE (en Test):  5030.63\n",
      "  MAE (en Test):  4075.14\n",
      "\n",
      ">>> F-Score (30 features) <<<\n",
      "  R2 (en Test):   0.5975\n",
      "  RMSE (en Test):  3510.04\n",
      "  MAE (en Test):  2665.89\n",
      "\n",
      ">>> Mutual_Info (30 features) <<<\n",
      "  R2 (en Test):   0.5703\n",
      "  RMSE (en Test):  3626.70\n",
      "  MAE (en Test):  2797.06\n",
      "\n",
      ">>> Lasso (30 features) <<<\n",
      "  R2 (en Test):   0.6009\n",
      "  RMSE (en Test):  3495.27\n",
      "  MAE (en Test):  2654.54\n",
      "\n",
      ">>> ElasticNet (30 features) <<<\n",
      "  R2 (en Test):   0.5920\n",
      "  RMSE (en Test):  3533.84\n",
      "  MAE (en Test):  2711.06\n",
      "\n",
      ">>> Random Forest (30 features) <<<\n",
      "  R2 (en Test):   0.6268\n",
      "  RMSE (en Test):  3380.04\n",
      "  MAE (en Test):  2565.61\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Un diccionario con todas tus listas de 30 features (de los pasos anteriores)\n",
    "listas_de_features = {\n",
    "    \"Varianza\": features_variance,\n",
    "    \"F-Score\": features_fscore,\n",
    "    \"Mutual_Info\": features_mi,\n",
    "    \"Lasso\": features_lasso,\n",
    "    \"ElasticNet\": features_elastic,\n",
    "    #\"Wrapper_SFS\": features_sfs,\n",
    "    \"Random Forest\": features_rf\n",
    "}\n",
    "\n",
    "def evaluar_en_test(X_train_subset, y_train, X_test_subset, y_test, nombre_metodo):\n",
    "    # --- A. Escalar (SIN DATA LEAKAGE) ---\n",
    "    # Limpiamos NaNs que puedan quedar\n",
    "    X_train_clean = X_train_subset.fillna(0)\n",
    "    X_test_clean = X_test_subset.fillna(0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_clean)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test_clean) \n",
    "    \n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred) \n",
    "    \n",
    "    # 5. Imprimir (como en tu código)\n",
    "    print(f\"\\n>>> {nombre_metodo} ({X_train_subset.shape[1]} features) <<<\")\n",
    "    print(f\"  R2 (en Test):   {r2:.4f}\")\n",
    "    print(f\"  RMSE (en Test):  {rmse:.2f}\")\n",
    "    print(f\"  MAE (en Test):  {mae:.2f}\") \n",
    "\n",
    "    \n",
    "for nombre, lista_de_features in listas_de_features.items():\n",
    "    X_train_subset = X_train[lista_de_features]\n",
    "    X_test_subset = X_test[lista_de_features]\n",
    "    \n",
    "    evaluar_en_test(X_train_subset, y_train, X_test_subset, y_test, nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a64e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6842055e",
   "metadata": {},
   "source": [
    "#### Elección de variables y creación de los df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22eec9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_ok = [c for c in features_rf if c in df_train_final.columns]\n",
    "\n",
    "# Avisar si alguna feature no aparece en df_train\n",
    "faltan = sorted(set(features_rf) - set(cols_ok))\n",
    "if faltan:\n",
    "    print(\"No encontré estas columnas en df_train:\", faltan)\n",
    "\n",
    "df_train = df_train_final.loc[:, cols_ok + ['y'] + ['dia']].copy()\n",
    "df_test = df_test_final.loc[:, cols_ok + ['y'] + ['dia']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e04f413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frio_ma7',\n",
       " 'Frio',\n",
       " 'ratio_Bodega_sobre_Sala_Maq',\n",
       " 'ratio_Sala_Maq_sobre_Bodega',\n",
       " 'Sala Maq (Kw)__Consolidado EExServicios (Kw)__Consolidado EE',\n",
       " 'Sala Maq (Kw)__Consolidado EE',\n",
       " 'Bodega_share',\n",
       " 'Sala Maq (Kw)__Consolidado EE_x_mes_cos',\n",
       " 'ratio_Servicios_sobre_Bodega',\n",
       " 'ratio_Bodega_sobre_Servicios',\n",
       " 'Servicios (Kw)__Consolidado EE_x_mes_cos',\n",
       " 'Sala_Maq_sum',\n",
       " 'Sala Maq (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE',\n",
       " 'Frio_lag7',\n",
       " 'Frio_ma3',\n",
       " 'Agua Planta (Hl)__Consolidado Agua_x_mes_cos',\n",
       " 'Sala Maq (Kw)__Consolidado EExKW Gral Planta__Consolidado EE',\n",
       " 'Sala Maq (Kw)__Consolidado EE_x_mes_sin',\n",
       " 'Aire L4 / Hl__Consolidado KPI',\n",
       " 'KW Gral Planta__Consolidado EE_x_mes_cos',\n",
       " 'Linea 3 (Kw)__Consolidado EE',\n",
       " 'Servicios (Kw)__Consolidado EE',\n",
       " 'Red L1 y L2__Consolidado Agua',\n",
       " 'Sala Maq (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua',\n",
       " 'Agua Linea 3/Hl__Consolidado KPI',\n",
       " 'Sala Maq (Kw)__Consolidado EE_x_dow_cos',\n",
       " 'CO 2 Linea 4 / Hl__Consolidado KPI',\n",
       " 'Red Paste L4__Consolidado Agua',\n",
       " 'Totalizador_Aire_Cocina__Consolidado Aire',\n",
       " 'ET Linea 3/Hl__Consolidado KPI']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea4d96",
   "metadata": {},
   "source": [
    "['Frio_ma7',\n",
    " 'Frio',\n",
    " 'ratio_Bodega_sobre_Sala_Maq',\n",
    " 'ratio_Sala_Maq_sobre_Bodega',\n",
    " 'Sala Maq (Kw)__Consolidado EExServicios (Kw)__Consolidado EE',\n",
    " 'Sala Maq (Kw)__Consolidado EE',\n",
    " 'Bodega_share',\n",
    " 'Sala Maq (Kw)__Consolidado EE_x_mes_cos',\n",
    " 'ratio_Servicios_sobre_Bodega',\n",
    " 'Servicios (Kw)__Consolidado EE_x_mes_cos',\n",
    " 'ratio_Bodega_sobre_Servicios',\n",
    " 'Sala_Maq_sum',\n",
    " 'Sala Maq (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE',\n",
    " 'Frio_lag7',\n",
    " 'Frio_ma3',\n",
    " 'Agua Planta (Hl)__Consolidado Agua_x_mes_cos',\n",
    " 'Sala Maq (Kw)__Consolidado EE_x_mes_sin',\n",
    " 'Sala Maq (Kw)__Consolidado EExKW Gral Planta__Consolidado EE',\n",
    " 'Aire L4 / Hl__Consolidado KPI',\n",
    " 'KW Gral Planta__Consolidado EE_x_mes_cos',\n",
    " 'Linea 3 (Kw)__Consolidado EE',\n",
    " 'Servicios (Kw)__Consolidado EE',\n",
    " 'Red L1 y L2__Consolidado Agua',\n",
    " 'Sala Maq (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua',\n",
    " 'Agua Linea 3/Hl__Consolidado KPI',\n",
    " 'Sala Maq (Kw)__Consolidado EE_x_dow_cos',\n",
    " 'CO 2 Linea 4 / Hl__Consolidado KPI',\n",
    " 'Red Paste L4__Consolidado Agua',\n",
    " 'ET Linea 3/Hl__Consolidado KPI',\n",
    " 'Totalizador_Aire_Cocina__Consolidado Aire']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a7d1d6",
   "metadata": {},
   "source": [
    "### Pipeline para escalado y transformacióon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8c7ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxcox_transform(X):\n",
    "    X = np.where(X <= 0, X + 1e-9, X)\n",
    "    return PowerTransformer(method='box-cox', standardize=True).fit_transform(X)\n",
    "\n",
    "def log_transform(X):\n",
    "    return np.log1p(np.clip(X, a_min=0, a_max=None))\n",
    "\n",
    "def sqrt_transform(X):\n",
    "    return np.sqrt(np.abs(X))\n",
    "\n",
    "\n",
    "ycol = 'y'\n",
    "non_features = [ycol, 'dia'] \n",
    "\n",
    "# 1a. Crear X_train, y_train (para ENTRENAR el GridSearchCV)\n",
    "X_train_full = df_train.drop(columns=non_features, errors='ignore')\n",
    "y_train = df_train[ycol]\n",
    "\n",
    "# 1b. Crear X_test, y_test (para EVALUAR AL FINAL)\n",
    "X_test_full = df_test.drop(columns=non_features, errors='ignore')\n",
    "y_test = df_test[ycol]\n",
    "\n",
    "# 1c. Detectar features numéricas (APRENDIENDO SOLO DE X_TRAIN)\n",
    "numeric_features = X_train_full.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# 1d. Asegurarnos que X_train y X_test tengan las mismas columnas numéricas\n",
    "X_train = X_train_full[numeric_features].fillna(0)\n",
    "X_test = X_test_full[numeric_features].fillna(0) # Rellenamos NaNs simple\n",
    "\n",
    "# --- 2. DEFINICIÓN DEL PIPELINE (Tu código, sin cambios) ---\n",
    "# (El imputer SimpleImputer(median) es bueno para velocidad en el gridsearch)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', 'passthrough'),\n",
    "    ('power', 'passthrough')\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge()) # Ponemos un modelo default, se va a reemplazar\n",
    "])\n",
    "\n",
    "# --- 3. ESPACIO DE BÚSQUEDA (Tu código, sin cambios) ---\n",
    "param_grid = {\n",
    "    'preprocessor__num__scaler': [ \n",
    "        'passthrough',\n",
    "        StandardScaler(),\n",
    "        MinMaxScaler(),\n",
    "        RobustScaler()\n",
    "    ],\n",
    "    \n",
    "    \n",
    "    'preprocessor__num__power': [ \n",
    "        PowerTransformer(method='yeo-johnson'),\n",
    "        FunctionTransformer(boxcox_transform, validate=False),\n",
    "        FunctionTransformer(log_transform, validate=False),\n",
    "        FunctionTransformer(sqrt_transform, validate=False),\n",
    "        'passthrough'\n",
    "    ],\n",
    "    \n",
    "\n",
    "    'regressor': [\n",
    "        XGBRegressor(\n",
    "            n_estimators=600,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.0,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\"\n",
    "        ),\n",
    "        RandomForestRegressor(\n",
    "            n_estimators=600,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features=\"sqrt\",\n",
    "            bootstrap=True,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        ),\n",
    "        LGBMRegressor(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            max_depth=-1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        Ridge(\n",
    "            alpha=1.0,\n",
    "            fit_intercept=True\n",
    "        ),\n",
    "        Lasso(\n",
    "            alpha=0.001,\n",
    "            max_iter=10000,\n",
    "            fit_intercept=True\n",
    "        )\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb883ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "75 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 655, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 996, in fit_transform\n",
      "    result = self._call_func_on_transformers(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 897, in _call_func_on_transformers\n",
      "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 82, in __call__\n",
      "    return super().__call__(iterable_with_config_and_warning_filters)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\joblib\\parallel.py\", line 1986, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\joblib\\parallel.py\", line 1914, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 147, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 731, in fit_transform\n",
      "    return last_step.fit_transform(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\", line 897, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\", line 263, in transform\n",
      "    out = self._transform(X, func=self.func, kw_args=self.kw_args)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\", line 390, in _transform\n",
      "    return func(X, **(kw_args if kw_args else {}))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_10108\\1689583320.py\", line 3, in boxcox_transform\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 3337, in fit_transform\n",
      "    return self._fit(X, y, force_transform=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 3340, in _fit\n",
      "    X = self._check_input(X, in_fit=True, check_positive=True)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 3581, in _check_input\n",
      "    raise ValueError(\n",
      "ValueError: The Box-Cox transformation can only be applied to strictly positive data\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Usuario\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [-2998.70910054 -2883.55743449 -3043.5596246  -2847.71251762\n",
      " -2859.26992278 -2998.70910054 -2883.81811303 -3051.45298511\n",
      " -2868.85049709 -2868.16124566 -2998.70910054 -2883.61298738\n",
      " -3022.1062661  -2841.31293808 -2850.26182701 -2998.70910054\n",
      " -2884.0255004  -3047.25579673 -2886.552413   -2893.86382134\n",
      "            nan            nan            nan            nan\n",
      "            nan            nan            nan            nan\n",
      "            nan            nan            nan            nan\n",
      "            nan            nan            nan            nan\n",
      "            nan            nan            nan            nan\n",
      " -3012.21268431 -2920.49336391 -3070.39465715 -2970.42980021\n",
      " -3048.06435675 -3079.1904063  -2940.42185087 -3174.83363217\n",
      " -2931.10833157 -2913.76048736 -2998.70910054 -2883.97795821\n",
      " -3071.01177    -2846.84653093 -2872.89845953 -2999.47849148\n",
      " -2916.99324796 -3150.85506609 -2890.39324148 -2876.01470683\n",
      " -3018.79045698 -2915.1675948  -3129.96228706 -2931.70821452\n",
      " -2923.84607401 -3166.29843021 -3133.46091839 -3246.20457449\n",
      " -5117.24246405 -5111.03389664 -2998.01819714 -2885.23500589\n",
      " -3070.91322127 -2855.4539602  -2904.13102898 -3176.24711191\n",
      " -3027.33377747 -3284.63154076 -3538.37323133 -3474.83388755\n",
      " -2997.64405148 -2893.22470715 -3053.27511556 -2860.8791154\n",
      " -2861.69820685 -2998.70910054 -2884.0934831  -3073.95942696\n",
      " -2861.54194427 -2861.25476775 -2998.70910054 -2883.95778178\n",
      " -3070.67232959 -2852.20220216 -2861.25923299 -2978.30916013\n",
      " -2891.86666175 -3056.14773111 -2856.46583221 -2861.25194819]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Los mejores parámetros encontrados (en train) son: {'preprocessor__num__power': PowerTransformer(), 'preprocessor__num__scaler': MinMaxScaler(), 'regressor': Ridge()}\n",
      "El mejor MAE (en CV de train) es: -2841.3129\n",
      "R² (en Test):  0.5968\n",
      "MAE (en Test): 2744.29\n",
      "RMSE (en Test): 52.39\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=42) # Tu CV original\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model_pipeline, \n",
    "    param_grid, \n",
    "    cv=cv, # Reemplaza por tscv si quieres probar\n",
    "    scoring='neg_mean_absolute_error', # O 'neg_mean_absolute_error' para el MAE\n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir los mejores parámetros (encontrados en Train)\n",
    "print(\"\\nLos mejores parámetros encontrados (en train) son:\", grid_search.best_params_)\n",
    "print(\"El mejor MAE (en CV de train) es: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_pred) # La métrica que te pide el TP\n",
    "mse_test  = mean_squared_error(y_test, y_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(f\"R² (en Test):  {r2_test:.4f}\")\n",
    "print(f\"MAE (en Test): {mae_test:.2f}\")\n",
    "print(f\"RMSE (en Test): {rmse_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e15f20c",
   "metadata": {},
   "source": [
    "### Cargar datos escalados y transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b9a6265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separando Train / Val en la fecha: 2022-07-01\n",
      "Forma df_train_noval: (724, 32)\n",
      "Forma df_val: (182, 32)\n",
      "Forma df_test_final: (297, 32)\n",
      "\n",
      "Se identificaron 30 columnas numéricas para escalar.\n",
      "Forma X_train_noval_numeric: (724, 30)\n",
      "Forma X_val_numeric: (182, 30)\n",
      "Forma X_test_numeric: (297, 30)\n",
      "\n",
      "Ajustando (fit) el pipeline SÓLO en X_train_noval_numeric...\n",
      "Transformando X_train_noval, X_val, y X_test...\n",
      "\n",
      "¡Linaje de datos guardado en ../data/data_lineage.json!\n",
      "\n",
      "¡Listo! Archivos COMPLETOS y SIN LEAKAGE guardados:\n",
      "Train escalado: ../data/dataset_train_final.csv (Shape: (724, 31))\n",
      "Val escalado:   ../data/dataset_val_final.csv (Shape: (182, 31))\n",
      "Test escalado:  ../data/dataset_test_final.csv (Shape: (297, 31))\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Imports (Asegúrate de tenerlos) ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from pandas.api.types import is_numeric_dtype # Para chequear tipos\n",
    "\n",
    "# --- (ASUMIMOS QUE YA CARGASTE ESTOS 2 OBJETOS) ---\n",
    "# df_train: El dataframe COMPLETO (train+val) con 'dia', 'y', y features numéricas\n",
    "# df_test: El dataframe COMPLETO (test) con 'dia', 'y', y features numéricas\n",
    "# (Ignoramos las variables X_train y X_test viejas, estaban sucias)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# --- 1. DEFINIR CORTE Y SEPARAR DATAFRAMES (Tu código, está perfecto) ---\n",
    "CUTOFF_DATE = '2022-07-01'\n",
    "print(f\"Separando Train / Val en la fecha: {CUTOFF_DATE}\")\n",
    "\n",
    "\n",
    "# Separamos el df_train original en \"solo train\" y \"solo val\"\n",
    "df_train_noval = df_train[df_train['dia'] <= CUTOFF_DATE].copy()\n",
    "df_val = df_train[df_train['dia'] > CUTOFF_DATE].copy()\n",
    "df_test_final = df_test.copy() # df_test ya estaba separado\n",
    "\n",
    "print(f\"Forma df_train_noval: {df_train_noval.shape}\")\n",
    "print(f\"Forma df_val: {df_val.shape}\")\n",
    "print(f\"Forma df_test_final: {df_test_final.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. OBTENER LOS SETS NUMÉRICOS (X) SIN LEAKAGE (LA CORRECCIÓN) ---\n",
    "\n",
    "# ¡IMPORTANTE! Define aquí tu columna objetivo y otras que NO se escalan\n",
    "# (Ej. 'Frio' es el target crudo, 'y' es el target transformado)\n",
    "TARGET = 'y'\n",
    "COLS_NO_NUMERICAS = ['dia', TARGET] # ¡AJUSTA ESTO SI ES NECESARIO!\n",
    "\n",
    "# Identificamos las columnas numéricas (features) dinámicamente\n",
    "# \"col es numérica\" Y \"col no está en la lista de excluidas\"\n",
    "numeric_cols = [\n",
    "    col for col in df_train_noval.columns \n",
    "    if is_numeric_dtype(df_train_noval[col]) and col not in COLS_NO_NUMERICAS\n",
    "]\n",
    "print(f\"\\nSe identificaron {len(numeric_cols)} columnas numéricas para escalar.\")\n",
    "\n",
    "# Ahora creamos los 3 sets numéricos (X) a partir de los dataframes separados\n",
    "# Esta vez, SÍ va a funcionar porque numeric_cols viene de df_train_noval\n",
    "X_train_noval_numeric = df_train_noval[numeric_cols]\n",
    "X_val_numeric = df_val[numeric_cols]\n",
    "X_test_numeric = df_test_final[numeric_cols]\n",
    "\n",
    "print(f\"Forma X_train_noval_numeric: {X_train_noval_numeric.shape}\")\n",
    "print(f\"Forma X_val_numeric: {X_val_numeric.shape}\")\n",
    "print(f\"Forma X_test_numeric: {X_test_numeric.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. Definir el pipeline de preprocesamiento (Tu código) ---\n",
    "preproc_pipeline = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('power', PowerTransformer(method='yeo-johnson')) \n",
    "])\n",
    "\n",
    "\n",
    "# --- 4. APLICAR (FIT Y TRANSFORM) SIN LEAKAGE (Tu código, ahora funciona) ---\n",
    "print(\"\\nAjustando (fit) el pipeline SÓLO en X_train_noval_numeric...\")\n",
    "\n",
    "# ¡AQUÍ ESTÁ LA MAGIA! Fiteamos SOLO con \"solo train\"\n",
    "preproc_pipeline.fit(X_train_noval_numeric)\n",
    "\n",
    "# Ahora transformamos los 3 sets con el mismo pipeline (ya fiteado)\n",
    "print(\"Transformando X_train_noval, X_val, y X_test...\")\n",
    "X_train_noval_scaled_array = preproc_pipeline.transform(X_train_noval_numeric)\n",
    "X_val_scaled_array = preproc_pipeline.transform(X_val_numeric)\n",
    "X_test_scaled_array = preproc_pipeline.transform(X_test_numeric)\n",
    "\n",
    "\n",
    "# --- 5. CONSTRUIR LOS 3 DATAFRAMES FINALES ---\n",
    "# (Esta parte estaba bien, usará los nuevos arrays)\n",
    "\n",
    "# --- df_train_final ---\n",
    "df_train_final = df_train_noval.copy()\n",
    "df_train_scaled_features = pd.DataFrame(\n",
    "    X_train_noval_scaled_array, \n",
    "    columns=numeric_cols, \n",
    "    index=df_train_noval.index\n",
    ")\n",
    "df_train_final.update(df_train_scaled_features)\n",
    "df_train_final = df_train_final.drop(columns=['dia'])\n",
    "\n",
    "# --- df_val_final ---\n",
    "df_val_final = df_val.copy()\n",
    "df_val_scaled_features = pd.DataFrame(\n",
    "    X_val_scaled_array, \n",
    "    columns=numeric_cols, \n",
    "    index=df_val.index\n",
    ")\n",
    "df_val_final.update(df_val_scaled_features)\n",
    "df_val_final = df_val_final.drop(columns=['dia'])\n",
    "\n",
    "# --- df_test_final ---\n",
    "# (df_test_final ya existe, solo actualizamos las features)\n",
    "df_test_scaled_features = pd.DataFrame(\n",
    "    X_test_scaled_array, \n",
    "    columns=numeric_cols, \n",
    "    index=df_test_final.index\n",
    ")\n",
    "df_test_final.update(df_test_scaled_features)\n",
    "df_test_final = df_test_final.drop(columns=['dia'])\n",
    "\n",
    "# --- 6. Guardar en CSV (3 archivos) ---\n",
    "ruta_train_csv = '../data/dataset_train_final.csv'\n",
    "ruta_val_csv = '../data/dataset_val_final.csv'\n",
    "ruta_test_csv = '../data/dataset_test_final.csv'\n",
    "\n",
    "df_train_final.to_csv(ruta_train_csv, index=False)\n",
    "df_val_final.to_csv(ruta_val_csv, index=False)\n",
    "df_test_final.to_csv(ruta_test_csv, index=False)\n",
    "\n",
    "# --- 7. Guardar Linaje (Actualizado) ---\n",
    "info_linaje = {\n",
    "    'descripcion': 'Datasets finales procesados, escalados SIN LEAKAGE, y separados en train/val/test.',\n",
    "    'fuentes_creadas': [ruta_train_csv, ruta_val_csv, ruta_test_csv],\n",
    "    'script_transformacion': 'notebooks/preprocesamiento.ipynb', \n",
    "    'fecha_creacion': datetime.datetime.now().isoformat(),\n",
    "    'cutoff_date_train_val': CUTOFF_DATE\n",
    "}\n",
    "\n",
    "ruta_json = '../data/data_lineage.json'\n",
    "with open(ruta_json, 'w') as f:\n",
    "    json.dump(info_linaje, f, indent=4)\n",
    "\n",
    "print(f\"\\n¡Linaje de datos guardado en {ruta_json}!\")\n",
    "\n",
    "print(f\"\\n¡Listo! Archivos COMPLETOS y SIN LEAKAGE guardados:\")\n",
    "print(f\"Train escalado: {ruta_train_csv} (Shape: {df_train_final.shape})\")\n",
    "print(f\"Val escalado:   {ruta_val_csv} (Shape: {df_val_final.shape})\")\n",
    "print(f\"Test escalado:  {ruta_test_csv} (Shape: {df_test_final.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e3309f",
   "metadata": {},
   "source": [
    "### Chechksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f52a9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta 'c:\\Users\\out6let\\Desktop\\REPOSITORIO\\TPF\\src' agregada al path.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "# 2. Agrega 'src' al path de Python (si no está ya)\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    print(f\"Carpeta '{src_path}' agregada al path.\")\n",
    "\n",
    "from tools import checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deec39bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(906, 32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "410b6f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Checksum calculado y guardado: 9e0c562bf4ba40a3479bc90ce0858817'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checksum(df_train_final, \"df_train\")  \n",
    "checksum(df_test_final, \"df_test\")\n",
    "checksum(df_val_final, \"df_val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cervecera_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
