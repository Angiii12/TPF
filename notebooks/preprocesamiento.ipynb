{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b33cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, RobustScaler, FunctionTransformer\n",
    "from sklearn.linear_model import LassoCV, Ridge, Lasso\n",
    "from sklearn.feature_selection import f_regression, RFE, SelectKBest, SequentialFeatureSelector, RFECV\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a925fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(\"..\", \"data\", \"dataset_preprocesamiento_train.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"..\", \"data\", \"dataset_preprocesamiento_test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8b869",
   "metadata": {},
   "source": [
    "## Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6288c036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inicial: (913, 118) | Test inicial: (297, 118)\n",
      "Aprendiendo 'top5' features solo de df_train...\n",
      "Top 5 features aprendidas: ['Sala Maq (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EE', 'KW Gral Planta__Consolidado EE', 'Planta (Kw)__Consolidado EE', 'Agua Planta (Hl)__Consolidado Agua']\n",
      "DataFrame unificado para procesar: (1210, 118)\n",
      "Buscando clima de 2020-07-01 a 2023-10-25...\n",
      "Filas eliminadas por NaNs de lags: 7. Quedan 1203 filas.\n",
      "\n",
      "--- ¡PROCESO COMPLETADO SIN DATA LEAKAGE! ---\n",
      "Shape Train final: (906, 196)\n",
      "Shape Test final:  (297, 196)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import re\n",
    "import requests\n",
    "\n",
    "print(f\"Train inicial: {df_train.shape} | Test inicial: {df_test.shape}\")\n",
    "\n",
    "\n",
    "# --- 1. \"APRENDER\" (FIT) SOLO DE DF_TRAIN ---\n",
    "# (El único paso con Data Leakage potencial, ahora es seguro)\n",
    "print(\"Aprendiendo 'top5' features solo de df_train...\")\n",
    "ycol = 'Frio' # Usamos el 'Frio' de hoy como base para correlacionar\n",
    "\n",
    "num_train = df_train.select_dtypes(include=[np.number]).copy()\n",
    "candidatas_train = [c for c in num_train.columns if c != ycol and c != 'y' and num_train[c].notna().any()]\n",
    "\n",
    "top5_features_aprendidas = (\n",
    "    num_train[candidatas_train].corrwith(num_train[ycol])\n",
    "    .abs()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(5)\n",
    "    .index.tolist()\n",
    ")\n",
    "print(f\"Top 5 features aprendidas: {top5_features_aprendidas}\")\n",
    "\n",
    "\n",
    "# --- 2. JUNTAR Y TRANSFORMAR (Aplicar Feature Engineering) ---\n",
    "\n",
    "# 2a. Juntamos los dataframes para que los lags/rolling fluyan\n",
    "split_date = df_test['dia'].min() # Guardamos la fecha de corte\n",
    "df_full = pd.concat([df_train, df_test]).sort_values(by='dia').reset_index(drop=True)\n",
    "\n",
    "print(f\"DataFrame unificado para procesar: {df_full.shape}\")\n",
    "\n",
    "# 2b. APLICAMOS TODO EL FEATURE ENGINEERING AL DATAFRAME JUNTO\n",
    "# (Tu código, pero aplicado a 'df_full' y con los cambios que pediste)\n",
    "\n",
    "# --- Lags y Rolling (Tus cambios) ---\n",
    "ycol = 'Frio'\n",
    "df_full[f'{ycol}_lag7'] = df_full[ycol].shift(7)\n",
    "\n",
    "y_obs = df_full[ycol].shift(1) # Frio de ayer\n",
    "df_full[f'{ycol}_ma3'] = y_obs.rolling(window=3, min_periods=1).mean()\n",
    "df_full[f'{ycol}_ma7'] = y_obs.rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "# --- Fechas Cíclicas y Finde ---\n",
    "df_full['fecha'] = pd.to_datetime(df_full['dia'], format='%Y-%m-%d', errors='coerce')\n",
    "dow = df_full['fecha'].dt.dayofweek\n",
    "df_full['dow_sin'] = np.sin(2*np.pi * dow / 7)\n",
    "df_full['dow_cos'] = np.cos(2*np.pi * dow / 7)\n",
    "m  = df_full['fecha'].dt.month\n",
    "m0 = (m - 1)\n",
    "df_full['mes_sin'] = np.sin(2*np.pi * m0 / 12)\n",
    "df_full['mes_cos'] = np.cos(2*np.pi * m0 / 12)\n",
    "df_full['fin_de_semana'] = df_full['fecha'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "# --- Ratios e Interacciones (usando la 'top5_features_aprendidas') ---\n",
    "top5 = top5_features_aprendidas # Usamos la variable segura\n",
    "for a, b in combinations(top5, 2):\n",
    "    df_full[f'{a}x{b}'] = df_full[a] * df_full[b]\n",
    "for cyc in ['dow_sin', 'dow_cos', 'mes_sin', 'mes_cos']:\n",
    "    if cyc in df_full.columns:\n",
    "        for c in top5:\n",
    "            df_full[f'{c}_x_{cyc}'] = df_full[c] * df_full[cyc]\n",
    "if 'fin_de_semana' in df_full.columns:\n",
    "    for c in top5:\n",
    "        df_full[f'{c}_x_finde'] = df_full[c] * df_full['fin_de_semana']\n",
    "\n",
    "# --- Ratios de Áreas ---\n",
    "AREAS = {\n",
    "    \"Elaboración\": r\"Elab|Elabor|Coci|Cocina|Mosto|Lauter|Macer|Paste\",\n",
    "    \"Envasado\":    r\"Envas|Llen|Linea|L[2345]\\b\",\n",
    "    \"Bodega\":      r\"Bodega|Bodeg\",\n",
    "    \"Servicios\":   r\"Servicios|Vapor|Gas|Agua|Aire|Caldera|Compres|Chiller|Sala\",\n",
    "    \"Sala_Maq\":    r\"Sala.*Maq\",\n",
    "}\n",
    "def safe_div(a, b):\n",
    "    # Usamos np.where para evitar dividir por cero\n",
    "    return np.where(b != 0, a / b, 0)\n",
    "\n",
    "num_cols = df_full.select_dtypes(include=[np.number]).columns\n",
    "area_cols = {}\n",
    "for area, pat in AREAS.items():\n",
    "    regex = re.compile(pat, flags=re.IGNORECASE)\n",
    "    cols = [c for c in num_cols if regex.search(c)]\n",
    "    area_cols[area] = cols\n",
    "\n",
    "area_sum = {}\n",
    "for area, cols in area_cols.items():\n",
    "    if cols:\n",
    "        df_full[f'{area}_sum'] = df_full[cols].sum(axis=1, skipna=True)\n",
    "        area_sum[area] = df_full[f'{area}_sum']\n",
    "    else:\n",
    "        df_full[f'{area}_sum'] = 0.0\n",
    "        area_sum[area] = df_full[f'{area}_sum']\n",
    "\n",
    "areas_presentes = list(area_sum.keys())\n",
    "total_sel = sum(area_sum[a] for a in areas_presentes)\n",
    "df_full['Consumo_Total_Areas'] = total_sel\n",
    "\n",
    "for a in areas_presentes:\n",
    "    df_full[f'{a}_share'] = safe_div(df_full[f'{a}_sum'], df_full['Consumo_Total_Areas'])\n",
    "for a, b in combinations(areas_presentes, 2):\n",
    "    df_full[f'ratio_{a}_sobre_{b}'] = safe_div(df_full[f'{a}_sum'], df_full[f'{b}_sum'])\n",
    "    df_full[f'ratio_{b}_sobre_{a}'] = safe_div(df_full[f'{b}_sum'], df_full[f'{a}_sum'])\n",
    "\n",
    "df_full = df_full.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# --- Estacionalidad y Clima ---\n",
    "m = df_full['fecha'].dt.month\n",
    "est_idx = np.select(\n",
    "    [m.isin([12, 1, 2]), m.isin([3, 4, 5]), m.isin([6, 7, 8]), m.isin([9, 10, 11])],\n",
    "    [0, 1, 2, 3], default=np.nan\n",
    ")\n",
    "df_full['estacion_sin'] = np.sin(2*np.pi * est_idx / 4)\n",
    "df_full['estacion_cos'] = np.cos(2*np.pi * est_idx / 4)\n",
    "\n",
    "start_date = df_full[\"fecha\"].min().date().isoformat()\n",
    "end_date   = df_full[\"fecha\"].max().date().isoformat()\n",
    "print(f\"Buscando clima de {start_date} a {end_date}...\")\n",
    "lat, lon = 32.56717, -116.62509\n",
    "url = (\n",
    "    \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    f\"?latitude={lat}&longitude={lon}\"\n",
    "    f\"&start_date={start_date}&end_date={end_date}\"\n",
    "    \"&daily=temperature_2m_mean\"\n",
    "    \"&timezone=auto\"\n",
    ")\n",
    "data = requests.get(url, timeout=60).json()\n",
    "wx_d = pd.DataFrame({\n",
    "    \"fecha\": pd.to_datetime(data[\"daily\"][\"time\"]),\n",
    "    \"t2m_mean_C\": data[\"daily\"][\"temperature_2m_mean\"],\n",
    "})\n",
    "df_full = df_full.merge(wx_d, on=\"fecha\", how=\"left\")\n",
    "\n",
    "\n",
    "# --- 3. LIMPIAR Y SEPARAR ---\n",
    "\n",
    "# 3a. Borrar las primeras 7 filas (como dijo tu profe)\n",
    "# Usamos .iloc[7:] para quedarnos con todo DESPUÉS de la fila 7\n",
    "df_processed = df_full.iloc[7:].copy()\n",
    "print(f\"Filas eliminadas por NaNs de lags: 7. Quedan {df_processed.shape[0]} filas.\")\n",
    "\n",
    "# 3b. Rellenar cualquier otro NaN que quede (del merge de clima, ratios, etc.)\n",
    "df_processed = df_processed.fillna(0) # O puedes usar un imputador aquí si prefieres\n",
    "\n",
    "# 3c. Volver a separar\n",
    "df_train_final = df_processed[df_processed['dia'] < split_date].copy()\n",
    "df_test_final = df_processed[df_processed['dia'] >= split_date].copy()\n",
    "\n",
    "\n",
    "print(\"\\n--- ¡PROCESO COMPLETADO SIN DATA LEAKAGE! ---\")\n",
    "print(f\"Shape Train final: {df_train_final.shape}\")\n",
    "print(f\"Shape Test final:  {df_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7857db",
   "metadata": {},
   "source": [
    "### Selección de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d256ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listos para seleccionar. 192 features numéricas iniciales.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression, mutual_info_regression, SequentialFeatureSelector, SelectFromModel\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (Asumo que df_train_final y df_test_final ya existen)\n",
    "\n",
    "# Definimos la columna objetivo (target)\n",
    "ycol = 'y'\n",
    "\n",
    "# --- Creamos X_train, y_train, X_test, y_test ---\n",
    "# (Esta vez, X_train y X_test se quedan con las columnas de texto por ahora)\n",
    "X_train_full = df_train_final.drop(columns=[ycol], errors='ignore')\n",
    "y_train = df_train_final[ycol]\n",
    "\n",
    "X_test_full = df_test_final.drop(columns=[ycol], errors='ignore')\n",
    "y_test = df_test_final[ycol]\n",
    "\n",
    "# --- ¡ESTE ES EL PASO CLAVE QUE FALTABA! ---\n",
    "# 1. Creamos una lista de features que SÍ son numéricas\n",
    "numeric_features = X_train_full.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# 2. Creamos los DataFrames X_train y X_test solo con esas columnas\n",
    "X_train = X_train_full[numeric_features].copy()\n",
    "X_test = X_test_full[numeric_features].copy()\n",
    "# (Nos aseguramos de que X_test tenga las mismas columnas en el mismo orden)\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "print(f\"Listos para seleccionar. {len(numeric_features)} features numéricas iniciales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319eaae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Método Varianza (k=30) --- \n",
      "['Unnamed: 0', 'Linea 3 (Kw)__Consolidado EE', 'Linea 2 (Kw)__Consolidado EE', 'Vapor L3__Consolidado GasVapor', 'Totalizador_Aire_L3__Consolidado Aire', 'Totalizador_Aire_L5__Consolidado Aire', 'dow_sin', 'dow_cos', 'mes_sin', 'mes_cos', 'fin_de_semana', 'KW Gral Planta__Consolidado EE_x_dow_sin', 'Planta (Kw)__Consolidado EE_x_dow_sin', 'Servicios (Kw)__Consolidado EE_x_dow_cos', 'KW Gral Planta__Consolidado EE_x_dow_cos', 'Planta (Kw)__Consolidado EE_x_dow_cos', 'Agua Planta (Hl)__Consolidado Agua_x_dow_cos', 'Servicios (Kw)__Consolidado EE_x_mes_sin', 'KW Gral Planta__Consolidado EE_x_mes_sin', 'Planta (Kw)__Consolidado EE_x_mes_sin', 'Agua Planta (Hl)__Consolidado Agua_x_mes_sin', 'KW Gral Planta__Consolidado EE_x_mes_cos', 'Planta (Kw)__Consolidado EE_x_mes_cos', 'Sala Maq (Kw)__Consolidado EE_x_finde', 'Servicios (Kw)__Consolidado EE_x_finde', 'KW Gral Planta__Consolidado EE_x_finde', 'Planta (Kw)__Consolidado EE_x_finde', 'Agua Planta (Hl)__Consolidado Agua_x_finde', 'estacion_sin', 'estacion_cos']\n",
      "\n",
      "--- 2. Método F-Score (k=30) --- \n",
      "['Sala Maq (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EE', 'KW Gral Planta__Consolidado EE', 'Planta (Kw)__Consolidado EE', 'Agua Planta (Hl)__Consolidado Agua', 'Frio', 'Frio_lag7', 'Frio_ma3', 'Frio_ma7', 'mes_cos', 'Sala Maq (Kw)__Consolidado EExServicios (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Servicios (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'Servicios (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'KW Gral Planta__Consolidado EExPlanta (Kw)__Consolidado EE', 'KW Gral Planta__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Planta (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Servicios (Kw)__Consolidado EE_x_mes_cos', 'KW Gral Planta__Consolidado EE_x_mes_cos', 'Agua Planta (Hl)__Consolidado Agua_x_mes_cos', 'Servicios_sum', 'Sala_Maq_sum', 'Consumo_Total_Areas', 'Bodega_share', 'ratio_Bodega_sobre_Servicios', 'ratio_Servicios_sobre_Bodega', 'ratio_Sala_Maq_sobre_Bodega']\n",
      "\n",
      "--- 3. Método Mutual Info (k=30) --- \n",
      "['Unnamed: 0', 'Sala Maq (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EE', 'KW Gral Planta__Consolidado EE', 'Planta (Kw)__Consolidado EE', 'Frio', 'Frio_ma3', 'Frio_ma7', 'Sala Maq (Kw)__Consolidado EExServicios (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Servicios (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'Servicios (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'KW Gral Planta__Consolidado EExPlanta (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EE_x_dow_cos', 'Servicios (Kw)__Consolidado EE_x_dow_cos', 'Sala Maq (Kw)__Consolidado EE_x_mes_cos', 'Servicios (Kw)__Consolidado EE_x_mes_cos', 'KW Gral Planta__Consolidado EE_x_mes_cos', 'Planta (Kw)__Consolidado EE_x_mes_cos', 'Servicios_sum', 'Sala_Maq_sum', 'Consumo_Total_Areas', 'Bodega_share', 'ratio_Bodega_sobre_Servicios', 'ratio_Servicios_sobre_Bodega', 'ratio_Bodega_sobre_Sala_Maq', 'ratio_Sala_Maq_sobre_Bodega']\n"
     ]
    }
   ],
   "source": [
    "# --- A. Filtro por Varianza (VarianceThreshold) ---\n",
    "# Primero escalamos, porque la varianza es sensible a la escala\n",
    "scaler_var = MinMaxScaler()\n",
    "\n",
    "# 1. APRENDER (fit) el scaler SOLO en X_train\n",
    "scaler_var.fit(X_train) \n",
    "\n",
    "# 2. APLICAR (transform) en ambos\n",
    "X_train_scaled_var = pd.DataFrame(scaler_var.transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled_var = pd.DataFrame(scaler_var.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# 3. APRENDER (fit) el selector de varianza SOLO en X_train_scaled\n",
    "# (Tu código buscaba k=16, pero ahora pedís k=30)\n",
    "# Vamos a usar SelectKBest con la varianza para pedir k=30\n",
    "var_selector = SelectKBest(score_func=lambda X, y: X.var(axis=0), k=30)\n",
    "var_selector.fit(X_train_scaled_var, y_train) # 'y_train' no se usa, pero la API lo pide\n",
    "\n",
    "# 4. OBTENER las 30 mejores features\n",
    "features_variance = X_train.columns[var_selector.get_support()]\n",
    "print(f\"\\n--- 1. Método Varianza (k=30) --- \\n{features_variance.tolist()}\")\n",
    "\n",
    "\n",
    "# --- B. Filtro por F-Score (f_regression) ---\n",
    "# (Este método necesita que los datos escalados no tengan NaNs)\n",
    "X_train_scaled_var = X_train_scaled_var.fillna(0)\n",
    "X_test_scaled_var = X_test_scaled_var.fillna(0)\n",
    "\n",
    "# 1. APRENDER (fit) el selector SOLO en X_train\n",
    "f_selector = SelectKBest(score_func=f_regression, k=30)\n",
    "f_selector.fit(X_train_scaled_var, y_train)\n",
    "\n",
    "# 2. OBTENER las 30 mejores features\n",
    "features_fscore = X_train.columns[f_selector.get_support()]\n",
    "print(f\"\\n--- 2. Método F-Score (k=30) --- \\n{features_fscore.tolist()}\")\n",
    "\n",
    "\n",
    "# --- C. Filtro por Información Mutua (mutual_info_regression) ---\n",
    "# 1. APRENDER (fit) el selector SOLO en X_train\n",
    "mi_selector = SelectKBest(score_func=mutual_info_regression, k=30)\n",
    "mi_selector.fit(X_train_scaled_var, y_train)\n",
    "\n",
    "# 2. OBTENER las 30 mejores features\n",
    "features_mi = X_train.columns[mi_selector.get_support()]\n",
    "print(f\"\\n--- 3. Método Mutual Info (k=30) --- \\n{features_mi.tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b83624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.957e+06, tolerance: 4.830e+06\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.414e+06, tolerance: 4.830e+06\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.720e+06, tolerance: 4.830e+06\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.107e+06, tolerance: 5.007e+06\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Método Lasso (top 30) --- \n",
      "['Frio', 'Frio_ma7', 'mes_cos', 'ratio_Bodega_sobre_Sala_Maq', 'ratio_Servicios_sobre_Bodega', 'mes_sin', 'estacion_cos', 'Red Paste L4__Consolidado Agua', 'ET Servicios / Hl__Consolidado KPI', 'Sala Maq (Kw)__Consolidado EE_x_dow_cos', 'Aire Envasado (M3)__Consolidado Aire', 'Agua Bodega / Hl__Consolidado KPI', 'KW Gral Planta__Consolidado EE', 'Aire Bodega / Hl__Consolidado KPI', 'Sala Maq (Kw)__Consolidado EE', 'Frio_lag7', 'Servicios (Kw)__Consolidado EE', 'Agua Linea 5/Hl__Consolidado KPI', 'ET Linea 3/Hl__Consolidado KPI', 'Agua Linea 4/Hl__Consolidado KPI', 'Agua Linea 2/Hl__Consolidado KPI', 'EE Cocina / Hl__Consolidado KPI', 'Unnamed: 0', 'ET Elab/Hl__Consolidado KPI', 'EE Linea 4 / Hl__Consolidado KPI', 'ET Bodega/Hl__Consolidado KPI', 'EE Caldera / Hl__Consolidado KPI', 'Aire L4 / Hl__Consolidado KPI', 'Agua Cocina / Hl__Consolidado KPI', 'EE Linea 2 / Hl__Consolidado KPI']\n",
      "\n",
      "--- 5. Método ElasticNet (top 30) --- \n",
      "['Frio_ma7', 'Frio_ma3', 'mes_cos', 'estacion_cos', 'Frio_lag7', 'Frio', 'Servicios (Kw)__Consolidado EE_x_mes_cos', 'KW Gral Planta__Consolidado EE_x_mes_cos', 'Agua Planta (Hl)__Consolidado Agua_x_mes_cos', 'Planta (Kw)__Consolidado EE_x_mes_cos', 'Sala Maq (Kw)__Consolidado EE', 'Sala Maq (Kw)__Consolidado EE_x_mes_cos', 'ratio_Sala_Maq_sobre_Bodega', 'Sala Maq (Kw)__Consolidado EExServicios (Kw)__Consolidado EE', 'ratio_Servicios_sobre_Bodega', 'Sala Maq (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'Sala_Maq_sum', 'Sala Maq (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 'Servicios (Kw)__Consolidado EE', 'ratio_Sala_Maq_sobre_Servicios', 'Consumo_Total_Areas', 'Sala_Maq_share', 'Servicios_share', 'Bodega_share', 'Servicios (Kw)__Consolidado EExKW Gral Planta__Consolidado EE', 'ratio_Bodega_sobre_Sala_Maq', 'Sala Maq (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua', 'Servicios_sum', 'Servicios (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE', 't2m_mean_C']\n"
     ]
    }
   ],
   "source": [
    "# --- D. Método Embebido (LassoCV) ---\n",
    "# Para Lasso/ElasticNet, es mejor usar StandardScaler (media 0, var 1)\n",
    "scaler_reg = StandardScaler()\n",
    "\n",
    "# 1. APRENDER (fit) el scaler SOLO en X_train\n",
    "scaler_reg.fit(X_train)\n",
    "\n",
    "# 2. APLICAR (transform) en ambos\n",
    "X_train_scaled_reg = pd.DataFrame(scaler_reg.transform(X_train.fillna(0)), columns=X_train.columns)\n",
    "X_test_scaled_reg = pd.DataFrame(scaler_reg.transform(X_test.fillna(0)), columns=X_test.columns)\n",
    "\n",
    "# 3. APRENDER (fit) el modelo Lasso SOLO en X_train\n",
    "lasso = LassoCV(cv=5, random_state=42).fit(X_train_scaled_reg, y_train)\n",
    "\n",
    "# 4. OBTENER las features (Lasso elige cuántas, no podemos forzar k=30)\n",
    "#    Así que tomaremos las 30 con coeficientes más altos (en valor absoluto)\n",
    "coef_lasso = pd.Series(lasso.coef_, index=X_train.columns)\n",
    "features_lasso = coef_lasso.abs().sort_values(ascending=False).head(30).index.tolist()\n",
    "print(f\"\\n--- 4. Método Lasso (top 30) --- \\n{features_lasso}\")\n",
    "\n",
    "\n",
    "# --- E. Método Embebido (ElasticNetCV) ---\n",
    "# 1. APRENDER (fit) el modelo ElasticNet SOLO en X_train\n",
    "elastic = ElasticNetCV(cv=5, random_state=42).fit(X_train_scaled_reg, y_train)\n",
    "\n",
    "# 2. OBTENER las 30 mejores features (mismo método que Lasso)\n",
    "coef_elastic = pd.Series(elastic.coef_, index=X_train.columns)\n",
    "features_elastic = coef_elastic.abs().sort_values(ascending=False).head(30).index.tolist()\n",
    "print(f\"\\n--- 5. Método ElasticNet (top 30) --- \\n{features_elastic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d5c6c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Método Wrapper (SFS) ---\n",
      "Iniciando SFS... (Esto puede tardar varios minutos)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 2. APRENDER (fit) el selector SFS SOLO en X_train\u001b[39;00m\n\u001b[0;32m     11\u001b[0m sfs \u001b[38;5;241m=\u001b[39m SequentialFeatureSelector(modelo_sfs, \n\u001b[0;32m     12\u001b[0m                               n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, \u001b[38;5;66;03m# ¡Aquí pones 30!\u001b[39;00m\n\u001b[0;32m     13\u001b[0m                               direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# 'forward' es más rápido que 'backward'\u001b[39;00m\n\u001b[0;32m     14\u001b[0m                               cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;66;03m# 3 splits es más rápido que 5\u001b[39;00m\n\u001b[0;32m     15\u001b[0m                               n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Usar todos los cores\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43msfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 3. OBTENER las 30 mejores features\u001b[39;00m\n\u001b[0;32m     20\u001b[0m features_sfs \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcolumns[sfs\u001b[38;5;241m.\u001b[39mget_support()]\n",
      "File \u001b[1;32mc:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\feature_selection\\_sequential.py:283\u001b[0m, in \u001b[0;36mSequentialFeatureSelector.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    281\u001b[0m     process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[1;32m--> 283\u001b[0m     new_feature_idx, new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_best_new_feature_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcloned_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_auto_select \u001b[38;5;129;01mand\u001b[39;00m ((new_score \u001b[38;5;241m-\u001b[39m old_score) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol):\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\feature_selection\\_sequential.py:314\u001b[0m, in \u001b[0;36mSequentialFeatureSelector._get_best_new_feature_score\u001b[1;34m(self, estimator, X, y, cv, current_mask, **params)\u001b[0m\n\u001b[0;32m    312\u001b[0m         candidate_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mcandidate_mask\n\u001b[0;32m    313\u001b[0m     X_new \u001b[38;5;241m=\u001b[39m X[:, candidate_mask]\n\u001b[1;32m--> 314\u001b[0m     scores[feature_idx] \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    323\u001b[0m new_feature_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m feature_idx: scores[feature_idx])\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_feature_idx, scores[new_feature_idx]\n",
      "File \u001b[1;32mc:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:677\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    675\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 677\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:399\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 399\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\angim\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- F. Método Wrapper (SequentialFeatureSelector) ---\n",
    "# (Usamos los datos escalados con StandardScaler, que ya están limpios de NaNs)\n",
    "print(\"\\n--- 6. Método Wrapper (SFS) ---\")\n",
    "print(\"Iniciando SFS... (Esto puede tardar varios minutos)...\")\n",
    "\n",
    "# 1. Definir el modelo que usará SFS para evaluar\n",
    "# (Tu código usaba GradientBoosting, es una buena elección)\n",
    "modelo_sfs = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# 2. APRENDER (fit) el selector SFS SOLO en X_train\n",
    "sfs = SequentialFeatureSelector(modelo_sfs, \n",
    "                              n_features_to_select=30, # ¡Aquí pones 30!\n",
    "                              direction='forward', # 'forward' es más rápido que 'backward'\n",
    "                              cv=3, # 3 splits es más rápido que 5\n",
    "                              n_jobs=-1) # Usar todos los cores\n",
    "\n",
    "sfs.fit(X_train_scaled_reg, y_train)\n",
    "\n",
    "# 3. OBTENER las 30 mejores features\n",
    "features_sfs = X_train.columns[sfs.get_support()]\n",
    "print(f\"SFS completado.\\nFeatures seleccionadas: \\n{features_sfs.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3d2d5",
   "metadata": {},
   "source": [
    "\n",
    "--- 6. Método Wrapper (SFS) ---\n",
    "Iniciando SFS... (Esto puede tardar varios minutos)...\n",
    "SFS completado.\n",
    "Features seleccionadas: \n",
    "['Agua Elab / Hl__Consolidado KPI', 'Agua Planta de Agua/Hl__Consolidado KPI', 'Produccion Agua / Hl__Consolidado KPI', 'EE Envasado / Hl__Consolidado KPI', 'Agua Bodega / Hl__Consolidado KPI', 'EE Elaboracion / Hl__Consolidado KPI', 'EE Linea 4 / Hl__Consolidado KPI', 'Aire Elaboracion / Hl__Consolidado KPI', 'KW Enfluentes Hidr__Totalizadores Energia', 'KW Obrador Contratistas__Totalizadores Energia', 'KW Mycom 3__Totalizadores Energia', 'KW Mycom 7__Totalizadores Energia', 'FC Lavadora L3__Consolidado Agua', 'Produccion (Hl)__Consolidado Agua', 'Frio', 'Frio_lag7', 'Frio_ma7', 'dow_sin', 'dow_cos', 'mes_sin', 'fin_de_semana', 'Servicios (Kw)__Consolidado EE_x_dow_sin', 'KW Gral Planta__Consolidado EE_x_dow_sin', 'KW Gral Planta__Totalizadores Energia_x_dow_sin', 'Planta (Kw)__Consolidado EE_x_dow_sin', 'KW Gral Planta__Consolidado EE_x_dow_cos', 'KW Gral Planta__Totalizadores Energia_x_dow_cos', 'Servicios (Kw)__Consolidado EE_x_finde', 'Bodega_sum', 'estacion_sin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62caa247",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "perm_train = permutation_importance(\n",
    "    rf, X_train, y_train,\n",
    "    n_repeats=5, random_state=42, n_jobs=-1,\n",
    "    scoring=\"neg_mean_absolute_error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "866c1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_imp_train = pd.Series(perm_train.importances_mean, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "features_rf = perm_imp_train.head(30).index.tolist()\n",
    "\n",
    "# Subsets coherentes en train/test\n",
    "X_train_sel = X_train[features_rf].copy()\n",
    "X_test_sel  = X_test[features_rf].copy()\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "rf_sel = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "rf_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "y_pred = rf_sel.predict(X_test_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f32a6893",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_sfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Un diccionario con todas tus listas de 30 features (de los pasos anteriores)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m listas_de_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVarianza\u001b[39m\u001b[38;5;124m\"\u001b[39m: features_variance,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF-Score\u001b[39m\u001b[38;5;124m\"\u001b[39m: features_fscore,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMutual_Info\u001b[39m\u001b[38;5;124m\"\u001b[39m: features_mi,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLasso\u001b[39m\u001b[38;5;124m\"\u001b[39m: features_lasso,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElasticNet\u001b[39m\u001b[38;5;124m\"\u001b[39m: features_elastic,\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrapper_SFS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mfeatures_sfs\u001b[49m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m\"\u001b[39m: features_rf\n\u001b[0;32m     20\u001b[0m }\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# (Tus dataframes X_train, y_train, X_test, y_test ya existen)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- INICIANDO EVALUACIÓN FINAL EN SET DE TEST ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features_sfs' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Definir el Modelo y los Sets de Features ---\n",
    "\n",
    "# El modelo que querés usar para la evaluación final\n",
    "model = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Un diccionario con todas tus listas de 30 features (de los pasos anteriores)\n",
    "listas_de_features = {\n",
    "    \"Varianza\": features_variance,\n",
    "    \"F-Score\": features_fscore,\n",
    "    \"Mutual_Info\": features_mi,\n",
    "    \"Lasso\": features_lasso,\n",
    "    \"ElasticNet\": features_elastic,\n",
    "    \"Wrapper_SFS\": features_sfs,\n",
    "    \"Random Forest\": features_rf\n",
    "}\n",
    "\n",
    "# (Tus dataframes X_train, y_train, X_test, y_test ya existen)\n",
    "print(\"--- INICIANDO EVALUACIÓN FINAL EN SET DE TEST ---\")\n",
    "\n",
    "\n",
    "# --- 2. La Función de Evaluación (simple, como la tuya) ---\n",
    "\n",
    "def evaluar_en_test(X_train_subset, y_train, X_test_subset, y_test, nombre_metodo):\n",
    "    \"\"\"\n",
    "    Entrena en Train, evalúa en Test y printea las métricas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- A. Escalar (SIN DATA LEAKAGE) ---\n",
    "    # Limpiamos NaNs que puedan quedar\n",
    "    X_train_clean = X_train_subset.fillna(0)\n",
    "    X_test_clean = X_test_subset.fillna(0)\n",
    "    \n",
    "    # 1. Aprender (fit) el scaler SOLO en Train\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_clean)\n",
    "    \n",
    "    # 2. Aplicar (transform) el scaler en Train y Test\n",
    "    X_test_scaled = scaler.transform(X_test_clean) # ¡Solo transform!\n",
    "    \n",
    "    \n",
    "    # --- B. Entrenar y Predecir ---\n",
    "    # 3. Entrenar el modelo\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 4. Predecir en Test\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    \n",
    "    # --- C. Calcular Métricas ---\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred) # El MAE que te pide el TP\n",
    "    \n",
    "    # 5. Imprimir (como en tu código)\n",
    "    print(f\"\\n>>> {nombre_metodo} ({X_train_subset.shape[1]} features) <<<\")\n",
    "    print(f\"  R2 (en Test):   {r2:.4f}\")\n",
    "    #print(f\"  MSE (en Test):  {mse:.2f}\")\n",
    "    print(f\"  MAE (en Test):  {mae:.2f}\") # ¡Métrica clave del TP!\n",
    "\n",
    "    \n",
    "# --- 3. Loop de Evaluación ---\n",
    "# (Esto reemplaza tu creación manual de X_l1, X_elastic, etc.)\n",
    "\n",
    "for nombre, lista_de_features in listas_de_features.items():\n",
    "    # Seleccionamos las 30 columnas para este método\n",
    "    X_train_subset = X_train[lista_de_features]\n",
    "    X_test_subset = X_test[lista_de_features]\n",
    "    \n",
    "    # Evaluamos\n",
    "    evaluar_en_test(X_train_subset, y_train, X_test_subset, y_test, nombre)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22eec9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_ok = [c for c in features_rf if c in df_train_final.columns]\n",
    "\n",
    "# Avisar si alguna feature no aparece en df_train\n",
    "faltan = sorted(set(features_rf) - set(cols_ok))\n",
    "if faltan:\n",
    "    print(\"No encontré estas columnas en df_train:\", faltan)\n",
    "\n",
    "# Sobrescribe con solo esas columnas\n",
    "df_train = df_train_final.loc[:, cols_ok + ['y']].copy()\n",
    "df_test = df_test_final.loc[:, cols_ok + ['y']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e04f413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frio_ma7',\n",
       " 'Frio',\n",
       " 'ratio_Bodega_sobre_Sala_Maq',\n",
       " 'ratio_Sala_Maq_sobre_Bodega',\n",
       " 'Sala Maq (Kw)__Consolidado EExServicios (Kw)__Consolidado EE',\n",
       " 'Sala Maq (Kw)__Consolidado EE',\n",
       " 'Bodega_share',\n",
       " 'Sala Maq (Kw)__Consolidado EE_x_mes_cos',\n",
       " 'ratio_Servicios_sobre_Bodega',\n",
       " 'Servicios (Kw)__Consolidado EE_x_mes_cos',\n",
       " 'ratio_Bodega_sobre_Servicios',\n",
       " 'Sala_Maq_sum',\n",
       " 'Sala Maq (Kw)__Consolidado EExPlanta (Kw)__Consolidado EE',\n",
       " 'Frio_lag7',\n",
       " 'Frio_ma3',\n",
       " 'Agua Planta (Hl)__Consolidado Agua_x_mes_cos',\n",
       " 'Sala Maq (Kw)__Consolidado EE_x_mes_sin',\n",
       " 'Sala Maq (Kw)__Consolidado EExKW Gral Planta__Consolidado EE',\n",
       " 'Aire L4 / Hl__Consolidado KPI',\n",
       " 'KW Gral Planta__Consolidado EE_x_mes_cos',\n",
       " 'Linea 3 (Kw)__Consolidado EE',\n",
       " 'Servicios (Kw)__Consolidado EE',\n",
       " 'Red L1 y L2__Consolidado Agua',\n",
       " 'Sala Maq (Kw)__Consolidado EExAgua Planta (Hl)__Consolidado Agua',\n",
       " 'Agua Linea 3/Hl__Consolidado KPI',\n",
       " 'Sala Maq (Kw)__Consolidado EE_x_dow_cos',\n",
       " 'CO 2 Linea 4 / Hl__Consolidado KPI',\n",
       " 'Red Paste L4__Consolidado Agua',\n",
       " 'ET Linea 3/Hl__Consolidado KPI',\n",
       " 'Totalizador_Aire_Cocina__Consolidado Aire']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listos. Usando 30 features numéricas.\n",
      "X_train shape: (906, 30), X_test shape: (297, 30)\n",
      "Iniciando la búsqueda (GridSearch) SOLO EN DF_TRAIN...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "75 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 655, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 996, in fit_transform\n",
      "    result = self._call_func_on_transformers(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 897, in _call_func_on_transformers\n",
      "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 82, in __call__\n",
      "    return super().__call__(iterable_with_config_and_warning_filters)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\joblib\\parallel.py\", line 1986, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\joblib\\parallel.py\", line 1914, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 147, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 731, in fit_transform\n",
      "    return last_step.fit_transform(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\", line 897, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\", line 263, in transform\n",
      "    out = self._transform(X, func=self.func, kw_args=self.kw_args)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\", line 390, in _transform\n",
      "    return func(X, **(kw_args if kw_args else {}))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\out6let\\AppData\\Local\\Temp\\ipykernel_9108\\1408175419.py\", line 5, in boxcox_transform\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 3337, in fit_transform\n",
      "    return self._fit(X, y, force_transform=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 3340, in _fit\n",
      "    X = self._check_input(X, in_fit=True, check_positive=True)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 3581, in _check_input\n",
      "    raise ValueError(\n",
      "ValueError: The Box-Cox transformation can only be applied to strictly positive data\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [-2909.38340125 -2888.24405641 -3008.93341604 -2817.21923133\n",
      " -2826.77214028 -2909.38340125 -2887.22178413 -3003.12930955\n",
      " -2844.96805865 -2842.56529606 -2909.38340125 -2887.43637895\n",
      " -3004.06290592 -2814.523349   -2813.3931594  -2909.38340125\n",
      " -2887.40733123 -3011.92107156 -2865.06332973 -2863.86331941\n",
      "            nan            nan            nan            nan\n",
      "            nan            nan            nan            nan\n",
      "            nan            nan            nan            nan\n",
      "            nan            nan            nan            nan\n",
      "            nan            nan            nan            nan\n",
      " -2971.74473324 -2908.51961184 -3007.50174156 -2918.15472718\n",
      " -2922.72584746 -3060.03854416 -2941.03676165 -3116.08760136\n",
      " -2882.08107233 -2884.3233437  -2909.38340125 -2887.98195874\n",
      " -3010.33089042 -2817.89250076 -2836.42949918 -3004.34761996\n",
      " -2916.52356052 -3075.31694226 -2875.3256407  -2882.91460142\n",
      " -2971.87777503 -2900.71182217 -3082.75275727 -2909.6661962\n",
      " -2908.4182342  -3215.86869376 -3172.52287087 -3399.18613431\n",
      " -5024.86610544 -5025.67973747 -2909.43913221 -2889.3463369\n",
      " -3012.51893481 -2828.96006466 -2879.42082848 -3094.98350346\n",
      " -3002.44671187 -3253.06323583 -3487.74469566 -3413.3678206\n",
      " -2925.25137848 -2886.36603451 -3010.78940748 -2830.54705925\n",
      " -2821.28224066 -2909.38340125 -2887.92490588 -3002.96225104\n",
      " -2823.88079146 -2824.10095046 -2909.38340125 -2887.61220566\n",
      " -3011.79330345 -2822.54676727 -2824.07065778 -2895.52895036\n",
      " -2885.70120302 -3032.62563476 -2823.14129293 -2824.10044794]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Los mejores parámetros encontrados (en train) son: {'preprocessor__num__power': PowerTransformer(), 'preprocessor__num__scaler': MinMaxScaler(), 'regressor': Lasso(alpha=0.001, max_iter=10000)}\n",
      "El mejor MAE (en CV de train) es: -2813.3932\n",
      "\n",
      "--- Evaluación Final en DF_TEST (Datos nunca vistos) ---\n",
      "R² (en Test):  0.6114\n",
      "MAE (en Test): 2685.65\n",
      "RMSE (en Test): 51.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\out6let\\miniconda3\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.653e+09, tolerance: 6.005e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# --- Tus funciones de transformación (BoxCox, Log, Sqrt) ---\n",
    "# (Las copio tal cual, están perfectas)\n",
    "def boxcox_transform(X):\n",
    "    X = np.where(X <= 0, X + 1e-9, X)\n",
    "    return PowerTransformer(method='box-cox', standardize=True).fit_transform(X)\n",
    "def log_transform(X):\n",
    "    return np.log1p(np.clip(X, a_min=0, a_max=None))\n",
    "def sqrt_transform(X):\n",
    "    return np.sqrt(np.abs(X))\n",
    "\n",
    "\n",
    "# --- 1. PREPARACIÓN DE DATOS (El cambio clave) ---\n",
    "# (Asumo que df_train y df_test ya existen y están limpios)\n",
    "\n",
    "ycol = 'y' # Tu columna objetivo (Frio shifteado)\n",
    "# Columnas que no son features (categóricas o de info)\n",
    "non_features = [ycol, 'dia'] \n",
    "\n",
    "# 1a. Crear X_train, y_train (para ENTRENAR el GridSearchCV)\n",
    "X_train_full = df_train.drop(columns=non_features, errors='ignore')\n",
    "y_train = df_train[ycol]\n",
    "\n",
    "# 1b. Crear X_test, y_test (para EVALUAR AL FINAL)\n",
    "X_test_full = df_test.drop(columns=non_features, errors='ignore')\n",
    "y_test = df_test[ycol]\n",
    "\n",
    "# 1c. Detectar features numéricas (APRENDIENDO SOLO DE X_TRAIN)\n",
    "numeric_features = X_train_full.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# 1d. Asegurarnos que X_train y X_test tengan las mismas columnas numéricas\n",
    "X_train = X_train_full[numeric_features].fillna(0)\n",
    "X_test = X_test_full[numeric_features].fillna(0) # Rellenamos NaNs simple\n",
    "\n",
    "print(f\"Listos. Usando {len(numeric_features)} features numéricas.\")\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. DEFINICIÓN DEL PIPELINE (Tu código, sin cambios) ---\n",
    "# (El imputer SimpleImputer(median) es bueno para velocidad en el gridsearch)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', 'passthrough'),\n",
    "    ('power', 'passthrough')\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge()) # Ponemos un modelo default, se va a reemplazar\n",
    "])\n",
    "\n",
    "\n",
    "# --- 3. ESPACIO DE BÚSQUEDA (Tu código, sin cambios) ---\n",
    "param_grid = {\n",
    "    # ¡DOBLE GUION BAJO!\n",
    "    'preprocessor__num__scaler': [ \n",
    "        'passthrough',\n",
    "        StandardScaler(),\n",
    "        MinMaxScaler(),\n",
    "        RobustScaler()\n",
    "    ],\n",
    "    \n",
    "    # ¡DOBLE GUION BAJO!\n",
    "    'preprocessor__num__power': [ \n",
    "        PowerTransformer(method='yeo-johnson'),\n",
    "        FunctionTransformer(boxcox_transform, validate=False),\n",
    "        FunctionTransformer(log_transform, validate=False),\n",
    "        FunctionTransformer(sqrt_transform, validate=False),\n",
    "        'passthrough'\n",
    "    ],\n",
    "    \n",
    "    # 'regressor' está bien con uno solo porque es un paso directo\n",
    "    # del 'model_pipeline'\n",
    "    'regressor': [\n",
    "        XGBRegressor(\n",
    "            n_estimators=600,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.0,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\"\n",
    "        ),\n",
    "        RandomForestRegressor(\n",
    "            n_estimators=600,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features=\"sqrt\",\n",
    "            bootstrap=True,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        ),\n",
    "        LGBMRegressor(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            max_depth=-1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        Ridge(\n",
    "            alpha=1.0,\n",
    "            fit_intercept=True\n",
    "        ),\n",
    "        Lasso(\n",
    "            alpha=0.001,\n",
    "            max_iter=10000,\n",
    "            fit_intercept=True\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# --- 4. GridSearchCV (EL FIT SE HACE SOLO EN TRAIN) ---\n",
    "# (Usamos TimeSeriesSplit para Cross-Validation, es mejor para series de tiempo)\n",
    "# tscv = TimeSeriesSplit(n_splits=5) # Opcional, pero recomendado\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42) # Tu CV original\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model_pipeline, \n",
    "    param_grid, \n",
    "    cv=cv, # Reemplaza por tscv si quieres probar\n",
    "    scoring='neg_mean_absolute_error', # O 'neg_mean_absolute_error' para el MAE\n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Iniciando la búsqueda (GridSearch) SOLO EN DF_TRAIN...\")\n",
    "\n",
    "# ¡AQUÍ ESTÁ LA CLAVE!\n",
    "# Entrenamos el buscador de modelos SOLO con los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir los mejores parámetros (encontrados en Train)\n",
    "print(\"\\nLos mejores parámetros encontrados (en train) son:\", grid_search.best_params_)\n",
    "print(\"El mejor MAE (en CV de train) es: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Guardamos el MEJOR pipeline encontrado\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "# --- 5. EVALUACIÓN FINAL (SOLO EN TEST) ---\n",
    "print(\"\\n--- Evaluación Final en DF_TEST (Datos nunca vistos) ---\")\n",
    "\n",
    "# Usamos el mejor pipeline para predecir en X_test\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "# Calcular métricas en Test\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_pred) # La métrica que te pide el TP\n",
    "rmse_test = np.sqrt(mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "print(f\"R² (en Test):  {r2_test:.4f}\")\n",
    "print(f\"MAE (en Test): {mae_test:.2f}\")\n",
    "print(f\"RMSE (en Test): {rmse_test:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c13dafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48372185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train (numérico): (906, 192)\n",
      "Forma de df_train (completo): (906, 31)\n",
      "\n",
      "Ajustando (fit) el pipeline ganador en X_train (numérico)...\n",
      "¡Linaje de datos guardado en ../data/data_lineage.json!\n",
      "\n",
      "¡Listo! Archivos COMPLETOS guardados:\n",
      "Train escalado: ../data/dataset_train_escalado.csv\n",
      "Test escalado:  ../data/dataset_test_escalado.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Definir el pipeline de preprocesamiento ganador ---\n",
    "preproc_pipeline = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('power', PowerTransformer(method='yeo-johnson')) \n",
    "])\n",
    "\n",
    "# --- 2. Asumir que X_train, y_train, df_train, df_test existen ---\n",
    "# X_train: SOLO las 30 features numéricas de train\n",
    "# y_train: La serie 'y' de train\n",
    "# df_train: El dataframe COMPLETO de train (con 'dia', 'y', 'Frio', etc.)\n",
    "# df_test: El dataframe COMPLETO de test\n",
    "\n",
    "print(f\"Forma de X_train (numérico): {X_train.shape}\")\n",
    "print(f\"Forma de df_train (completo): {df_train.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. Aplicar (FIT y TRANSFORM) en X_train (SIN LEAKAGE) ---\n",
    "print(\"\\nAjustando (fit) el pipeline ganador en X_train (numérico)...\")\n",
    "preproc_pipeline.fit(X_train)\n",
    "\n",
    "# Transformamos X_train y X_test\n",
    "X_train_escalado_array = preproc_pipeline.transform(X_train)\n",
    "X_test_escalado_array = preproc_pipeline.transform(X_test)\n",
    "\n",
    "\n",
    "# --- 4. CONSTRUIR LOS DATAFRAMES FINALES (EL PASO CLAVE) ---\n",
    "\n",
    "# --- df_train_escalado ---\n",
    "# 4a. Empezar con una copia del df_train original (que tiene 'dia', 'y', 'Frio', etc.)\n",
    "df_train_escalado_final = df_train.copy()\n",
    "\n",
    "# 4b. Crear un DataFrame con los datos escalados y los nombres de columna correctos\n",
    "df_train_scaled_features = pd.DataFrame(\n",
    "    X_train_escalado_array, \n",
    "    columns=X_train.columns, \n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# 4c. Actualizar las 30 columnas numéricas en la copia final con los valores escalados\n",
    "# (.update() reemplaza los valores de las columnas en un df con los valores de otro)\n",
    "df_train_escalado_final.update(df_train_scaled_features)\n",
    "\n",
    "\n",
    "# --- df_test_escalado ---\n",
    "# 4a. Empezar con una copia del df_test original\n",
    "df_test_escalado_final = df_test.copy()\n",
    "\n",
    "# 4b. Crear un DataFrame con los datos escalados\n",
    "df_test_scaled_features = pd.DataFrame(\n",
    "    X_test_escalado_array, \n",
    "    columns=X_test.columns, \n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 4c. Actualizar las 30 columnas numéricas en la copia final\n",
    "df_test_escalado_final.update(df_test_scaled_features)\n",
    "\n",
    "\n",
    "# --- 5. Guardar en CSV ---\n",
    "ruta_train_csv = '../data/dataset_train_escalado.csv'\n",
    "ruta_test_csv = '../data/dataset_test_escalado.csv'\n",
    "\n",
    "df_train_escalado_final.to_csv(ruta_train_csv, index=False)\n",
    "df_test_escalado_final.to_csv(ruta_test_csv, index=False)\n",
    "\n",
    "info_linaje = {\n",
    "    'descripcion': 'Dataset final procesado, con lags, features ciclicas y ratios.',\n",
    "    'fuente_original': 'data/dataset_train_escalado.csv y data/dataset_test_escalado.csv',\n",
    "    'script_transformacion': 'notebooks/preprocesamiento.ipynb', # O el nombre de tu script\n",
    "    'fecha_creacion': datetime.datetime.now().isoformat()\n",
    "    # (Buena práctica: añadir el hash del commit de Git)\n",
    "    # 'git_commit_hash': '...' \n",
    "}\n",
    "\n",
    "# 2b. Guardás el \"recibo\" en formato JSON\n",
    "ruta_json = '../data/data_lineage.json'\n",
    "with open(ruta_json, 'w') as f:\n",
    "    json.dump(info_linaje, f, indent=4)\n",
    "\n",
    "print(f\"¡Linaje de datos guardado en {ruta_json}!\")\n",
    "\n",
    "print(f\"\\n¡Listo! Archivos COMPLETOS guardados:\")\n",
    "print(f\"Train escalado: {ruta_train_csv}\")\n",
    "print(f\"Test escalado:  {ruta_test_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f52a9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta 'c:\\Users\\angim\\OneDrive\\Angi Cosas\\Universidad\\Laboratorio de datos II\\TPF\\src' agregada al path.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "# 2. Agrega 'src' al path de Python (si no está ya)\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    print(f\"Carpeta '{src_path}' agregada al path.\")\n",
    "\n",
    "from tools import checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deec39bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(913, 118)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "410b6f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Checksum calculado y guardado: 37387ee1ea2ebff9803aa80924233f28'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checksum(df_train_escalado_final, \"df_train\")  \n",
    "checksum(df_test_escalado_final, \"df_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cervecera_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
